---
title: "MAS8404_Project_210431461"
author: "210431461  |   21/10/22"
output: pdf_document
fontsize: 12pt
citation_package: natbib
bibliography: "references.bib"
biblio-style: "apalike"
link-citations: TRUE
---

\vspace{-0.7cm}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir= normalizePath('..'))
options(tinytex.verbose = TRUE)
```

```{r ProjectTemplate, include = FALSE}
library(ProjectTemplate)
load.project()
```

### Introduction 

At the University of Wisconsin Hospital, Dr. Wolberg (@Wolberg) collected breast tissue samples from women using fine needle aspiration cytology (FNAC)(@Project). Histological examination of the tissue collected by this procedure allows for a physician to determine whether or not the tissue is benign or malignant. Our objective is to build a classifier that determines whether a tissue sample is likely to be benign or malignant based on its cytological characteristics.

### Data Description

Dr. Wolberg reported his data chronologically to the `mlbench` R package (@Wolberg). The FNAC procedure allows for the identification of nine cytological characteristics (@Project): clump thickness (Cl.thickness / CT), uniformity of cell size (Cell.size / CS), uniformity of cell shape (Cell.shape / CSh), marginal adhesion (Marg.adhesion / MA), single epithelial cell size (Epith.c.size / E), bare nuclei (Bare.nuclei / BN), bland chromatin (Bl.chromatin / BC), normal nucleoli (Normal.nucleoli / NN) and mitoses (Mitoses / M). These characteristics for each tissue sample are measured on a discrete scale of one to ten, where smaller numbers indicate that the sample is healthier (@Project). To aid our analysis, the ordinal variables of this scale have been converted to quantitative variables.

This report explores data from a sample of 699 women in the `BreastCancer` data set; please note that 16 of the 699 observations have been removed due to missing attribute values. It is assumed that this is a random sample of women experiencing symptoms of breast cancer (@Project). Each woman is represented by a sample code number (Id) that reflects the chronological grouping of this data (@Wolberg). The data set also includes the result of further histological examination (Class), which confirms whether each woman's tissue sample was begin or malignant; this has been converted into a binary variable of 0 or 1, respectively.

### Data Exploration

Of the tissues samples from 683 women, 239 are confirmed as malignant; in our sample, 53.83% of the women who are experiencing breast cancer symptoms have malignant tissue. Whether the tissue is benign or malignant is considered the response variable, and the cytological characteristics of tissue are considered the predictor variables. The characteristics of malignant tissue tend to have higher numbers on the scale one to ten; this is visualized by the density distributions provided in `Appendix A`. All nine cytological characteristics of the tissue samples are presented below in a scatter plot matrix, where blue indicates the tissue is malignant.

```{r summmary_plot, echo=FALSE, fig.width=9, fig.height=9, fig.align='center'}
# Convert data into numeric matrix
BC2 <- data.matrix(BC1) 

# Extract the response variable
Class <- BC2[,11]-1

#Extract the predictor variables
x <- BC2[,2:10] 

# Quick pairs plot
pairs(x, col=c("black", "#1f78b4")[Class+1], main="Plot 1: Scatter Plot Matrix of All Cytological Characteristics")
```

This plot visualises the relationship between the characteristics; it suggests that there is a strong linear relationship between CS and CSh, indicating that larger cells have a more significant shape and more are likely to be malignant. There also appears to be a weak linear relationship between CS and CSh with E and BC, as well as a weak linear relationship between CT and CSh. Further investigation of these relationships would be beneficial. The correlations between other characteristics are not initially obvious and require deeper analysis. 

This is achieved by a sample correlation matrix, which quantifies the strength of the linear relationship.The correlation between all characteristics is visualised by the correlation plot below:

```{r summmary_cor, echo=FALSE, fig.width=5, fig.height=5, fig.align='center'}
# Observe the correlations
cor_bc <- cor(BC1[,2:10])

# Plot correlations
corrplot(cor_bc, order = "hclust", method = "circle", tl.col="black", tl.cex = 0.7, main="Plot 2: Cytological Characteristics Correlation", cex.main=0.8, mar=c(3, 2, 2, 0)) 
```

\vspace{-1.5cm}

In this plot, the correlation is more significant when the circle is both darker and larger. The plot confirms the strong linear relationship between CS and CSh, which suggests that any classifier is unlikely to need both characteristics. These characteristics also appear to have stronger relationships with CT, MA, E, BN, BC, and NN. M does not appear to have a significant correlation with any of the characteristics. These insights indicate that an accurate classifier may only need a selection of the predictor variables. 

Boxplots of each cytological characteristic are provided in `Appendix B`; these highlight that there is a distinctive difference between the characteristics of benign and malignant tissue. The mean, median and standard deviation (SD) of our data is, therefore, likely to be skewed unless the data is filtered by its Class (benign or malignant). The filtered summary statistics are presented in Table 1 on the next page.

```{r summary, include=FALSE}
# Create table
my.summary <- tribble(~"Variable", ~"CT", ~"CS", ~"CSh", ~"MA", ~"E", ~"BN", ~"BC", ~"NN", ~"M", "Median", 4.00, 1.00, 1.00, 1.00, 2.00, 1.00, 3.00, 1.00, 1.00, "Mean", 4.44, 3.15, 3.22, 2.83, 3.23, 2.54, 3.45, 2.87, 1.58, "SD", 2.82, 3.07, 2.99, 2.86, 2.22, 3.64, 2.45, 3.05, 1.64)

# Improve visuals of table
kable(my.summary, caption = "Summary Statistics of Cytological Characteristics") %>%
  kable_classic_2(full_width = F, latex_options = "HOLD_position")

#This table shows us that the mean is notably higher than the median for CS, CSh, BN and NN, suggesting that characteristics of malignant tissue are skewing the mean. The standard deviation is also quite large. 
```

```{r summary_MB, echo=FALSE}
# Create table
M_B <- tribble(~Variable, ~"Mean", ~"SD", ~"Median", ~"Mean", ~"SD", ~"Median", 
               "CT", "2.96", "1.67", "3.00", "7.19", "2.44",  "8.00",
               "CS", "1,31", "0.86", "1.00", "6.58", "2.72", "6.00",
               "CSh", "1.14", "0.96", "1.00", "6.56", "2.57", "6.00",
               "MA", "1.35", "0.92", "1.00", "5.59", "3.20", "5.00",
               "E", "2.11", "0.88", "2.00", "5.33", "2.44", "5.00",
               "BN", "1.35", "1.18", "1.00", "7.63", "3.12", "10.00",
               "BC", "2.08", "1.06", "2.00", "5.97", "2.28", "7.00",
               "NN", "1.26", "0.95", "1.00", "5.86", "3.35", "6.00", 
               "M", "1.07", "0.51", "1.00", "2.54", "2.40", "1.00")

# Improve visuals of table
kable(M_B, caption = "Summary Statistics by Class",  align = "c") %>%
  add_header_above(c(" " = 1, "Benign" = 3, "Malignant" = 3)) %>%
  kable_styling(full_width = F, latex_options = "HOLD_position")

```

This table clearly demonstrates that there is a significant difference between the characteristics of benign and malignant tissue. This is especially true for CT, BN and BC, which suggests that they may be the most important predictor variables. It is interesting that the SD of MA, BN and NN is noticeably larger than the other characteristics. If these characteristics are selected for the classifier, their variance may impact the accuracy of our classifier model.

```{r variation, include=FALSE}
# Sample variance of data
s <- var(BC2[,2:10]) # Removing Id and Class

# Calculate total variation
s_sq = diag(s) # Extract diagonal elements

(total_variation = sum(s_sq))
#70.7

# Generalised variance 
det(s) #47432
```

Given this, it is also important to explore how varied the data is for the cytological characteristics. There are two single measures of multivariate scatter that help us generalise this; these are the generalised variance, which for our data is 70.70, and the total variation, which for our data is 47432.00. The large values tell us that there is a high degree of scatter about the sample means of each variable. Although the data for all cytological characteristics is a common, discrete scale it may be beneficial to standardise our data so that it is centred.

```{r standardise, include=FALSE}
# Standardise the predictor variables 
xs <- scale(x)

# Sample means
center <- attr(xs, "scaled:center")

# Sample standard deviations
scale <- attr(xs, "scaled:scale")

# Reform the data frame
bc_data <- data.frame(xs, Class)
```

```{r pca, include=FALSE}
# Conduct PCA
pca <- prcomp(x=xs) # Already scaled, and excluding Class
print(pca) # Print results
summary(pca) # We need 6 PCAs to explain 0.93 of the variance, and 3 to explain 0.80.
```

To understand which characteristics most influence the variance, principal component analysis (PCA) was conducted. Three principal components account for 80.12% of the variance, and six account for 92.85% of the variance. The first principal component can be interpreted as an average measure of CS and CSh, which accounts for 65.60% of the variation. The second corresponds to M; together the first and second principal components account for 74.13% of the variation. The third principal component corresponds to CT. Although PCA is a distinct from best subset selection, it is interesting to note that CS, CSh, M and CT are the principal components of our data.

### Subset Selection

The exploratory data analysis suggests that some predictor variables are likely to be better at predicting our response variable than others. If this proves to be correct, this will allow us to learn the effects of fewer predictor variables more precisely. 

As our response variable is binary (benign or malignant), applying logistic regression is an appropriate approach to identify the best subset of predictor variables. After fitting the logistic regression model for `Class` to standardised data it is clear that CT, BN, MA and BC have a coefficient which is significantly different to zero when testing at the 5% level. This suggests that these characteristics will most contribute towards the classifier model.

```{r logfit, include=FALSE}
# Store n and p
n <- nrow(bc_data)
p <- ncol(bc_data)- 1

# Fit logistic regression model
(logr_fit = glm(Class ~ ., data = bc_data, family="binomial"))

# Summarise the model fit
summary(logr_fit) 
```

This result is broadly aligned to the variable selection performed by a LASSO regression. In the plot below, the last five variables to drop out are BN, CSh, CS, CT and NN; apart from CS, these predictor variables align with the results of the previous logistic regression. In the LASSO model, all the predictor values are retained as parameter estimates associated with the optimal value of the tuning parameter. These estimates highlight that CT, BN, CSh, BC, MA, and NN are the least shrunk towards zero and, therefore contribute the most.

```{r full_lasso, echo=FALSE, fig.height=4.5}
# Choose grid of values for the turning parameter
grid = 10^seq(5, -3, length = 100)

#Fit a model with LASSO penalty for each value of the turning parameter, for all data
lasso_fit = glmnet(xs, bc_data$Class, family = "binomial", alpha = 1, standardize = FALSE, lambda = grid)

# Examine the effect of the tuning parameter on the parameter estimates for all data
plot(lasso_fit, xvar="lambda", col=rainbow(p), label=TRUE, main="Plot 3: Effect of the Tuning Parameter on Parameter Estimates", cex.main=0.8, mar=c(3, 2, 2, 0))

# Cross-validate LASSO for all data
lasso_cv_fit = cv.glmnet(xs, bc_data$Class, family = "binomial", alpha = 1, standardize = FALSE, lambda = grid, type.measure = "class")

#Identify the optimal value for the turning parameter
lambda_lasso_min = lasso_cv_fit$lambda.min #0.007742637
which_lambda_lasso = which(lasso_cv_fit$lambda == lambda_lasso_min) #89

#Find the parameter associated with the optimal value of the turning parameter
optimal <- coef(lasso_fit, s=lambda_lasso_min)
```

To confirm how many predictor variables would be the best subset selection, the Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) are applied. The results are presented in the two plots on the next page.

```{r bss, echo=FALSE, message=FALSE, warning=FALSE, fig.height=4.5}
# Apply best subset selection
fit_AIC <- bestglm(bc_data, family = binomial, IC="AIC")
fit_BIC <- bestglm(bc_data, family = binomial, IC="BIC")

# Identify best-fitting models
best_AIC = fit_AIC$ModelReport$Bestk
best_BIC = fit_BIC$ModelReport$Bestk

# Create a multi-panel plotting device
par(mfrow=c(1,2))

# Produce plots, highlighting optimal value of 'k'
plot(0:p, fit_AIC$Subsets$AIC, xlab = "Number of Predictors", ylab = "AIC", type = "b", main = "Plot 4: AIC Selection", cex.main=0.8)
points(best_AIC, fit_AIC$Subsets$AIC[best_AIC+1], col="#1f78b4", pch=16)

plot(0:p, fit_BIC$Subsets$BIC, xlab = "Number of Predictors", ylab = "BIC", type = "b", main="Plot 5: BIC Selection", cex.main=0.8)
points(best_BIC, fit_BIC$Subsets$BIC[best_BIC+1], col="#1f78b4", pch=16)
```

In both plots, the blue dot is the number of predictor variables identified by the subset selection approach. It appears that a model with six variables is likely to be a good compromise between five and seven. The logistic regression subset selection determined that the cytological characteristics for these six variables are CT, CSh, MA, BN, BC, and NN. These align closely to the strongly correlated variables identified in our correlation matrix above.

```{r six, include=FALSE}
# It seems like the model with 6 predictors is a good compromise between 5 and 7 
pstar = 6

# Check which predictors are in the 6-predictor model
fit_AIC$Subsets[pstar+1,]
```

### Modelling

To increase the reliability of our approach towards building a classifier, the data is divided in two to allow for out-of-sample validation. In this project, 80% of the data is randomly allocated to a training data set to construct our classifier, with the remaining 20% becoming the testing data set to compute our test errors. The same test and train data sets are used for all the approaches in this section.

```{r train, include=FALSE}
# Construct a reduced data set containing only the selected predictors
(indices <- as.logical(fit_AIC$Subsets[pstar+1, 2:(p+1)]))
bc_data_red = data.frame(bc_data[,indices])

# Create test and train data sets
set.seed(683) 
train_index <- sample(nrow(bc_data_red), size = round(0.80 * nrow(bc_data_red)), replace = FALSE)
train <- bc_data_red[train_index,]
test <- bc_data_red[-train_index,]

# Obtain regression coefficients for this model
logr1_fit = glm(Class ~ ., data = train, family="binomial")
summary(logr1_fit)  # Estimate std. are the maximum liklihood estimates of the regression coefficients. Because Cl. thickness and bare.nuclei are the largest positive values, this indicates that leisons with higher numbers in theres are more likely to have malignant cancer?
summ(logr1_fit, scale = TRUE) # Presents details of model fit
summ(logr1_fit, confint = TRUE, digits = 3) # Presents confidence intervals
```
*Logistic Regression*

Logistic regression is used to assign observations to discrete response variables. When applied to our the data of our six predictor variables, the maximum likelihood estimates of the regression coefficients are presented on the next page.

```{r log_coefficients, echo=FALSE}
# Create table
logr_coefficients <- tribble(~"", ~"Intercept", ~"CT", ~"CSh", ~"MA", ~"BN", ~"BC", ~"NN", "Estimates", -1.28, 2.08, 1.14, 1.39, 1.69, 1.52, 0.87)

# Improve visuals
kable(logr_coefficients, caption = "Estimates of Regression Coefficients for Logistic Regression") %>%
  kable_classic_2(full_width = F, latex_options = "HOLD_position")
```

```{r log_error, include = FALSE}
# Training error
p1 <- predict(logr1_fit, train, type="response") 
(y = as.numeric(ifelse(p1 > 0.5, 1, 0))) # Would be likely to have malignant 

# Calculate fitted values 
phat <- predict(logr1_fit, train, type = "response") # Compute predicted probabilities
yhat <- ifelse(phat > 0.5, 1, 0) # Compute predicted values

#Compute the confusion matrix
(confusion <- table(Observed=train$Class, predicted=yhat))

# Normalise function
normalise = function(x) {
  return(x / sum(x))
}

# Apply function to the confusion matrix
t(apply(confusion, 1, normalise)) # Performance is okay.# Perform prediction 2

# Calculate the training error
1 - sum(diag(confusion)) / sum(confusion) # 0.02014652 (2.02%)

# Calculate Test error
logr_test = predict(logr1_fit, test, type ="response") 
yhat_logr_test = ifelse(logr_test > 0.5, 1, 0)

#Compute the confusion matrix
(confusion_2 <- table(Observed=test$Class, predicted=yhat_logr_test))

# Apply function to the confusion matrix
t(apply(confusion_2, 1, normalise)) # Performance is okay.

# Compute test error
(1-mean(test$Class == yhat_logr_test)) #0.05847953 (5.85%)

#Compute the confusion matrix
(confusion_2 <- table(Observed=test$Class, predicted=yhat_logr_test))

# Apply function to the confusion matrix
t(apply(confusion_2, 1, normalise)) # Performance is okay.
```

With 'in-sample validation', the training error is 2.02%. This is not very interesting as only the test error measures how well the method performs on previously unseen data (@Project). When using our train and test data sets for out-of-sample validation, the test error is 5.85%. This is slightly larger than the training error, which may indicate that the number of predictor variables included may benefit from being adjusted 

A helpful way to fairly compare our models is the Receiver Operating Characteristic (ROC) curve; this plot shows the performance of our classification model at different classification thresholds (@ROC). It is visualised below: 

```{r log_roc, echo=FALSE, message=FALSE, warning=FALSE, fig.height=4}
# Evaluate the model
pred.logr <- prediction(logr_test, test$Class)
roc.perf.logr = performance(pred.logr, measure = "tpr", x.measure = "fpr")
auc.train.logr <- performance(pred.logr, measure = "auc")
auc.train.logr <- auc.train.logr@y.values

# Plot logr on ROC
plot(roc.perf.logr, col="#1f78b4", main="Plot 6: Logisitic Regresion ROC", cex.main=0.8)
abline(a=0, b= 1)
text(x = .25, y = .65, paste("AUC = ", round(auc.train.logr[[1]],3), sep = ""))
```

The measure of accuracy is the area under the ROC curve (AUC), which provides an aggregate measure of performance (@ROC). This curve is far from the diagonal and is quite close to the perfect accuracy area of 1.00. Helpfully, the plot specifies the accuracy of our model as 0.985, and this indicates the performance quite good.

*LASSO Regression*

Regularisation methods, such as LASSO, are shrinkage methods; they work to minimise the loss function and shrink the maximum likelihood estimates of regression coefficients to zero. LASSO performs subset selection in addition to shrinkage; given that the test error was higher than the training error for logistic regression, this method is selected over ridge regression. The cross-validation scores of the LASSO regression applied to the same data as above is visualised below:

```{r LASSOcv, echo=FALSE, message=FALSE, warning=FALSE}
# Extract the response variable
tClass <- train[,7]

#Extract the predictor variables
xt <- train[,1:6]

##Cross-validate LASSO for training data
train_l = as.matrix(xt) #Covert to matrix for cv
lasso_cv_fit_train = cv.glmnet(train_l, tClass, family = "binomial", alpha = 1, standardize = FALSE, lambda = grid, type.measure = "class")
plot(lasso_cv_fit_train, main = "Plot 7: LASSO Regression Cross-Validation Scores", cex.main=0.8, mar=c(3, 2, 2, 0)) # Plot cross-validation

```

```{r LASSO, include = FALSE}

##Identify the optimal value for the turning parameter
lambda_lasso_min_train = lasso_cv_fit_train$lambda.min #0.002104904
which_lambda_lasso_train = which(lasso_cv_fit_train$lambda == lambda_lasso_min_train) #96

# Extract corresponding mean MSE
MSE_train <- lasso_cv_fit_train$cvm[which_lambda_lasso_train] # 0.02380952

##Fit a model with LASSO penalty for each value of the turning parameter to training data
lasso_fit_train = glmnet(xt, tClass, family = "binomial", alpha = 1, standardize = FALSE, lambda = lambda_lasso_min_train)

##Compute fitted values for the validation data
xtest <- test[,1:6] ##Extract the predictor variables
test_l <- as.matrix(xtest) #Test data as matrix
lasso_phat_test = predict(lasso_fit_train, test_l, s=lambda_lasso_min_train, type="response")
lasso_yhat_test = ifelse(lasso_phat_test > 0.5, 1, 0)

##Compute test error
1-mean(test$Class == lasso_yhat_test) #0.05839416, 5.84%
```

For each value of $\lambda$, this plot shows us the mean squared error (MSE) across the default 10 *k*-folds. The grey bars at each point show the mean plus or minus one standard error. The first dashed line shows us the location of the minimum $\lambda$, and the second shows the optimal value for $\lambda$. The minimum for our data the minimum is 0.00210490 and the MSE is 0.02380952. The regression coefficients obtained by performing the LASSO with the optimal turning parameter are shown in the table below:

```{r lasso_coefficients, echo=FALSE}
# Find the parameter estimates associated with optimal value of the tuning parameter
optimal_train <- coef(lasso_fit_train, s=lambda_lasso_min_train)

# Create table
lasso_coefficients <- tribble(~"", ~"Intercept", ~"CT", ~"CSh", ~"MA", ~"BN", ~"BC", ~"NN", "Estimates", -1.07, 1.71, 1.07, 1.11, 1.52, 1.25, 0.746)

# Improve visuals
kable(lasso_coefficients, caption = "Estimates of Regression Coefficients for LASSO Regression") %>%
  kable_classic_2(full_width = F, latex_options = "HOLD_position")
```

When using our train and test data sets for out-of-sample validation, the test error is 5.85%. This is the same test error as the logistic regression. The full test error value for the logistic regression is 0.05847953 and, for the LASSO regression, it is 0.05839416. This shows us that the LASSO model is very slightly more accurate.

The accuracy of our model is also visualised by the ROC curve on the next page. 

```{r lasso_ROC, echo=FALSE, fig.height=4}
## Evaluate the model
lasso_phat_test = predict(lasso_fit_train, test_l, s=lambda_lasso_min_train, type="response") 
pred.lasso <- prediction(lasso_phat_test, test$Class)
roc.perf.lasso = performance(pred.lasso, measure = "tpr", x.measure = "fpr")
auc.train.lasso <- performance(pred.lasso, measure = "auc")
auc.train.lasso <- auc.train.lasso@y.values

# Plot LASSO on ROC
plot(roc.perf.lasso, col="#1f78b4", main="Plot : LASSO Regression ROC", cex.main=0.8)
abline(a=0, b= 1)
text(x = .25, y = .65, paste("AUC = ", round(auc.train.lasso[[1]],3), sep = ""))
```

Again, this plot indicates the performance of our model is quite good; the accuracy of our model is 0.985 - the same as that for the logistic regression. 

*Discriminant Analysis*

Discriminant analysis is a technique used to classify observations into groups that do not overlap (@LDA). Linear Discriminant Analysis (LDA) assumes that the common covariance matrix of the predictor variables is the same. Quadratic Discriminant Analysis (QDA) does not assume this. However, in practice, whether the common covariance is generally unknown. 

The density distributions in `Appendix A` and boxplots in `Appendix B` suggest that our data is not normally distributed and indicates non-equal variances (respectively). QDA is likely to be more appropriate than LDA. The group means generated by both LDA and QDA are the same, and so only one version of the group means is presented in the table below:

```{r groupmeans, echo=FALSE}
# Perform LDA 
lda_train <- lda(Class~., data=train, type = "response")

# Perform QDA
qda_train <- qda(Class~., data=train)

# Create table
DA <- tribble(~"", ~"CT", ~"CSh", ~"MA", ~"BN", ~"BC", ~"NN",
              "Benign", -0.56, -0.61, -0.53, -0.61, -0.57, -0.53, 
              "Malignant", 0.95, 1.13, 0.96, 1.15, 1.03, 0.99) 

# Improve visuals of table
kable(DA, caption = "LDA and QDA Group Means",  align = "c") %>%
  kable_styling(full_width = F, latex_options = "HOLD_position")
```

```{r DA_test_error, include=FALSE}
## Compute fitted values for the lda validation data
lda_test = predict(lda_train, test)
lda_yhat_test = lda_test$class 

## Compute test error
1 - mean(test$Class == lda_yhat_test) # 0.0729927, 7.30%

## Compute fitted values for the qda validation data
qda_test = predict(qda_train, test, type = "response")
yhat_test = qda_test$class

## Compute test error
1 - mean(test$Class == yhat_test) # 0.06569343, 6.57%
```

When using our train and test data sets for out-of-sample validation, the test error for LDA is 7.30% and QDA is 6.57%. As expected QDA has the lower test error. 

The accuracy of our LDA model is 0.984; it is visualised by the ROC curve on the next page. 

```{r lda_ROC, echo=FALSE, fig.height=4}
# Get the posteriors as a dataframe.
lda.predict.posteriors <- as.data.frame(lda_test$posterior)

# Evaluate the model
pred.lda <- prediction(lda.predict.posteriors[,2], test$Class)
roc.perf.lda = performance(pred.lda, measure = "tpr", x.measure = "fpr")
auc.train.lda <- performance(pred.lda, measure = "auc")
auc.train.lda <- auc.train.lda@y.values
# Plot
plot(roc.perf.lda, col="#1f78b4", main="Plot 8: LDA ROC", cex.main=0.8)
abline(a=0, b= 1)
text(x = .25, y = .65, paste("AUC = ", round(auc.train.lda[[1]],3), sep = ""))
```

The accuracy of our QDA model 0.987 and is visualised by the ROC curve below:

```{r qda_ROC, echo=FALSE, fig.height=4}
## Get the posteriors as a dataframe
qda.predict.posteriors <- as.data.frame(qda_test$posterior)

## Evaluate the model
pred.qda <- prediction(qda.predict.posteriors[,2], test$Class)
roc.perf.qda = performance(pred.qda, measure = "tpr", x.measure = "fpr")
auc.train.qda <- performance(pred.qda, measure = "auc")
auc.train.qda <- auc.train.qda@y.values

# Plot QDA on ROC
plot(roc.perf.qda, col="#1f78b4", main="Plot : QDA ROC", cex.main=0.8)
abline(a=0, b= 1)
text(x = .25, y = .65, paste("AUC = ", round(auc.train.qda[[1]],3), sep = ""))
```

Interestingly, the ROC curve suggests the QDA model is the most accurate despite. As its test error is larger than either of the regression this warrants further investigation.


### Evaluation

Based on the predictive performance of the models, the LASSO regression has the lowest test error of 0.05839416. This is almost negligibly smaller than the logistic regression as both test errors round to 5.85%. These are followed by QDA (6.57%) and LDA (7.30%). The validation set approach (splitting the data into two) was applied to fairly assess and compare the supervised learning techniques. As smaller test errors indicate more accurate models, the findings outlined by this report suggest that the LASSO regression would be the best classifier. 

The ROC curves and AUC results also help fairly assess and compare the models. The logistic and LASSO regressions have an AUC of 0.985, the LDA AUC is 0.984 and the QDA AUC is 0.987. This indicates the QDA model is the most accurate, making it the better classifier. However, as this result contracts to it's test error of 6.57%, further investigation is needed to determine why there are conflicting results; for example, code error. 

None of the classifier models include all nine cytological characteristics. Using the methods of AIC and BIC it was determined that six predictor variables was likely to provide the most accurate model. Through logistic regression best subset selection, it was determined these variables should be: CT, CSh, MA, BN, BC, and NN. This result aligned to the subset selection of a LASSO regression applied to the data set of nine cytological characteristics. It is interesting that the LASSO regression performed on the train data set did conduct indicate the need for any further subset selection as none of the regression coefficients were completely shrunk to zero. However, the logistic regression test error was larger than its training error. These two insights indicate further experimentation with the predictor variables would be beneficial to improve the accuracy of our classifier model.

All of the models applied in this report have test error rates that would be impractical in a clinical setting; the test errors are far too large. Further work is needed to improve the accuracy of the models; this could include different methods of best subset selection, dividing the test and train data differently (and perhaps only using cross-validation *k*-folds), and, ideally, access to a much larger data set. 

\newpage
### Appendix A: Denisty Distributions of Cytological Characteristics by Class
```{r density, echo=FALSE, fig.height=11.5, fig.width=9, message=FALSE, warning=FALSE}
# Density Distribution of Clump Thickness
CT <- BC1 %>%
  ggplot(aes(x = Cl.thickness, colour = Class)) + geom_density(kernel = "gaussian", size = 1.5) + labs(x = "Clump Thickness", y = "Density", legend = "Class") + scale_color_brewer(palette = "Paired") + theme(axis.title.y=element_blank())

# Density Distribution of Cell Size
CS <- BC1 %>%
  ggplot(aes(x = Cell.size, colour = Class)) + geom_density(kernel = "gaussian", size = 1.5) + labs(x = "Cell Size", y = "Density", legend = "Class") + scale_color_brewer(palette = "Paired") + theme(axis.title.y=element_blank())

# Density Distribution
CSh <- BC1 %>%
  ggplot(aes(x = Cell.shape, colour = Class)) + geom_density(kernel = "gaussian", size = 1.5) + labs(x = "Cell shape", y = "Density", legend = "Class") + scale_color_brewer(palette = "Paired") + theme(axis.title.y=element_blank())

# Density Distribution of Marginal Adhesion
MA <- BC1 %>%
  ggplot(aes(x = Marg.adhesion, colour = Class)) + geom_density(kernel = "gaussian", size = 1.5) + labs(x = "Marginal Adhesion", y = "Density", legend = "Class") + scale_color_brewer(palette = "Paired") + theme(axis.title.y=element_blank())

# Density Distribution of Single Epithelial Cell Size
E <- BC1 %>%
  ggplot(aes(x = Epith.c.size, colour = Class)) + geom_density(kernel = "gaussian", size = 1.5) + labs(x = "Single Epithelial Cell Size", y = "Density", legend = "Class") + scale_color_brewer(palette = "Paired") + theme(axis.title.y=element_blank())

# Density Distribution of Bare Nuclei
BN <- BC1 %>%
  ggplot(aes(x = Bare.nuclei, colour = Class)) + geom_density(kernel = "gaussian", size = 1.5) + labs(x = "Bare Nuclei", y = "Density", legend = "Class") + scale_color_brewer(palette = "Paired") + theme(axis.title.y=element_blank())

# Density Distribution of Bland Chromatin
BC <- BC1 %>%
  ggplot(aes(x = Bl.cromatin, colour = Class)) + geom_density(kernel = "gaussian", size = 1.5) + labs(x = "Bland Chromatin", y = "Density", legend = "Class") + scale_color_brewer(palette = "Paired") + theme(axis.title.y=element_blank())

# Density Distribution of Normal Nucleoli
NN <- BC1 %>%
  ggplot(aes(x = Normal.nucleoli, colour = Class)) + geom_density(kernel = "gaussian", size = 1.5) + labs(x = "Normal Nucleoli", y = "Density", legend = "Class") + scale_color_brewer(palette = "Paired") + theme(axis.title.y=element_blank())

# Density Distribution of Mitoses
M <- BC1 %>%
  ggplot(aes(x = Mitoses, colour = Class)) + geom_density(kernel = "gaussian", size = 1.5) + labs(x = "Mitoses", y = "Density", legend = "Class") + scale_color_brewer(palette = "Paired") + theme(axis.title.y=element_blank())

# Visualise plots as grid. 
TQ <- ggarrange(CT, CS, CSh, MA, E, BN, BC, NN, M, ncol=3, nrow=3, common.legend = TRUE, legend="top")
annotate_figure(TQ, left = text_grob("Density", rot = 90, vjust = 1))
```

\newpage
### Appendix B: Boxplots of Cytological Characteristics by Class
```{r boxplots, echo=FALSE, fig.height=11.5, fig.width=9}

B_CT <- ggplot(BC1, aes(x = Class, y = Cl.thickness)) + geom_boxplot(alpha = 0.3, color = "#1f78b4") + labs(x = "Class", y = "Clump Thickness (scale)", title = "Clump Thickness by Class") + theme(legend.position = "none") + expand_limits(y = 0)

B_CS <- ggplot(BC1, aes(x = Class, y = Cell.size)) + geom_boxplot(alpha = 0.3, color = "#1f78b4") + labs(x = "Class", y = "Cell Size (scale)", title = "Cell Size by Class") + theme(legend.position = "none") + expand_limits(y = 0)

B_CSh <- ggplot(BC1, aes(x = Class, y = Cell.shape)) + geom_boxplot(alpha = 0.3, color = "#1f78b4") + labs(x = "Class", y = "Cell Shape (scale)", title = "Cell Shape by Class") + theme(legend.position = "none") + expand_limits(y = 0)

B_MA <- ggplot(BC1, aes(x = Class, y = Marg.adhesion)) + geom_boxplot(alpha = 0.3, color = "#1f78b4") + labs(x = "Class", y = "Marginal Adhesion (scale)", title = "Marginal Adhesion  by Class") + theme(legend.position = "none") + expand_limits(y = 0)

B_E <- ggplot(BC1, aes(x = Class, y = Epith.c.size)) + geom_boxplot(alpha = 0.3, color = "#1f78b4") + labs(x = "Class", y = "Single Epitheial Cell Size (scale)", title = "Single Epitheial Cell Size by Class") + theme(legend.position = "none") + expand_limits(y = 0)

B_BN <- ggplot(BC1, aes(x = Class, y = Bare.nuclei)) + geom_boxplot(alpha = 0.3, color = "#1f78b4") + labs(x = "Class", y = "Bare Nuclei (scale)", title = "Bare Nuclei by Class") + theme(legend.position = "none") + expand_limits(y = 0)

B_BC <- ggplot(BC1, aes(x = Class, y = Bl.cromatin)) + geom_boxplot(alpha = 0.3, color = "#1f78b4") + labs(x = "Class", y = "Bland Cromatin (scale)", title = "Bland Cromatin by Class") + theme(legend.position = "none") + expand_limits(y = 0)

B_NN <- ggplot(BC1, aes(x = Class, y = Normal.nucleoli)) + geom_boxplot(alpha = 0.3, color = "#1f78b4") + labs(x = "Class", y = "Normal Nucleoli (scale)", title = "Normal Nucleoli by Class") + theme(legend.position = "none") + expand_limits(y = 0)

B_M <- ggplot(BC1, aes(x = Class, y = Mitoses)) + geom_boxplot(alpha = 0.3, color = "#1f78b4") + labs(x = "Class", y = "Mitoses (scale)", title = "Mitoses by Class") + theme(legend.position = "none") + expand_limits(y = 0)

# Visualise plots as grid. 
ggarrange(B_CT, B_CS, B_CSh, B_MA, B_E, B_BN, B_BC, B_NN, B_M, ncol=3, nrow=3, common.legend = TRUE, legend="top")
```

\newpage

# Bibliography