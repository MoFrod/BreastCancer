---
title: "MAS8404_Project_210431461"
author: "210431461  |   21/10/22"
output: pdf_document
fontsize: 12pt
citation_package: natbib
bibliography: "references.bib"
biblio-style: "apalike"
link-citations: TRUE
---

\vspace{-0.7cm}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir= normalizePath('..'))
options(tinytex.verbose = TRUE)
```

```{r ProjectTemplate, include = FALSE}
library(ProjectTemplate)
load.project()
```

### Introduction 

At the University of Wisconsin Hospital, Dr. Wolberg (@Wolberg) collected breast tissue samples from women using fine needle aspiration cytology (FNAC)(@Project). Histological examination of the tissue collected by this procedure allows for the physician to determine whether or not it is benign or malignant. Our objective is to build a classifier based on the cytological characteristics that determines whether a tissue sample is likely to be benign or malignant.

### Data Description

Dr. Wolberg reported his data chronologically to the `mlbench` R package (@Wolberg). The FNAC procedure allows for the identification of nine cytological characteristics (@Project): clump thickness (Cl.thickness / CT), uniformity of cell size (Cell.size / CS), uniformity of cell shape (Cell.shape / CSh), marginal adhesion (Marg.adhesion / MA), single epithelial cell size (Epith.c.size / E), bare nuclei (Bare.nuclei / BN), bland chromatin (Bl.chromatin / BC), normal nucleoli (Normal.nucleoli / NN) and mitoses (Mitoses M). These characteristics for each tissue sample are measured on a discrete scale of one to ten, where smaller numbers indicate that the sample is healthier (@Project). To aid our analysis, the ordinal variables of this scale have been converted to quantitative variables.

This report explores data from a sample of 699 women in the `BreastCancer` data set; please note that 16 of the 699 observations have been removed due to missing attribute values. It is assumed that this is a random sample of women experiencing symptoms of breast cancer (@Project). Each woman is represented by a sample code number (Id) that reflects the chronological grouping of this data (@Wolberg). The data set also includes the result of further histological examination (Class), which confirms whether each woman's tissue sample was begin or malignant; this has been converted into a binary variable of 0 or 1, respectively.

### Data Exploration

Of the tissues samples from 683 women, 239 are confirmed as malignant; in our sample, 53.83% of the women who are experiencing breast cancer symptoms have malignant tissue. Whether the tissue is benign or malignant is considered the response variable, and the cytological characteristics of tissue are considered as the predictor variables. The characteristics of malignant tend to have higher numbers on the scale one to ten; this is visualized by the density distributions provided in `Appendix A`. All nine cytological characteristics of the tissue samples are presented below in a scatter plot matrix, where blue indicates the tissue is malignant.

```{r summmary_plot, echo=FALSE, fig.width=9, fig.height=9, fig.align='center'}
# Convert data into numeric matrix
BC2 <- data.matrix(BC1) 

# Extract the response variable
Class <- BC2[,11]-1

#Extract the predictor variables
x <- BC2[,2:10] 

# Quick pairs plot
pairs(x, col=c("black", "#1f78b4")[Class+1], main="Plot 1: Scatter Plot Matrix of All Cytological Characteristics")
```

This plot visualises the relationship between the characteristics; it suggests that there is a strong relationship between CS and CSh, indicating that larger cells have a more significant shape and are likely to be malignant. Whether this relationship is causal or correlated is worthy of further investigation. 

The correlations between other characteristics are not initially obvious and require deeper analysis. A sample correlation matrix quantifies the strength of a linear relationship between the characteristics, and is visualised by the correlation plot below.

```{r summmary_cor, echo=FALSE, fig.width=5, fig.height=5, fig.align='center'}
# Observe the correlations
cor_bc <- cor(BC1[,2:10])

# Plot correlations
corrplot(cor_bc, order = "hclust", method = "circle", tl.col="black", tl.cex = 0.7, main="Plot 2: Cytological Characteristics Correlation", cex.main=0.8, mar=c(3, 2, 2, 0)) 
```

\vspace{-1.5cm}

In this plot, the correlation is more significant when the circle is both darker and larger. This plot confirms the strong linear relationship between cell shape and cell size, which suggests that any regression model is unlikely to need both. These characteristics also appear to have stronger relationships with CT, MA, E, BN, BC, and NN. M does not appear to have a significant correlation with any of the characteristics. This indicates that a selection of the predictor variables could be used to build an accurate classifier. 

```{r general_variance, include=FALSE}
# Sample variance of data
s <- var(BC2)

# Generalised variance 
det(s) #55382860
```

```{r total_variation, include=FALSE}
# Sample variance of data
s <- var(BC2)

# Calculate total variation
s_sq = diag(s) # Extract diagonal elements

(total_variation = sum(s_sq))
#33283.24
```

The two single measures of multivariate scatter help us generalise how our data varies: generalised variance and total variation. For our data, the generalised variance is 55382860.00 and the total variation is 33283.24. This tells us there is a high degree of scatter about the sample means of each variable, highlighting that it will be important to transform our data so that it is standardised, putting all the variables on a common scale. 

The mean, median and standard deviation for the nine cytological characteristics of the tissue samples are also presented on the next page. 

```{r summary, echo=FALSE}
# Create table
my.summary <- tribble(~"Variable", ~"CT", ~"CS", ~"CSh", ~"MA", ~"E", ~"BN", ~"BC", ~"NN", ~"M", "Median", 4.00, 1.00, 1.00, 1.00, 2.00, 1.00, 3.00, 1.00, 1.00, "Mean", 4.44, 3.15, 3.22, 2.83, 3.23, 2.54, 3.45, 2.87, 1.58, "SD", 2.82, 3.07, 2.99, 2.86, 2.22, 3.64, 2.45, 3.05, 1.64)

# Improve visuals of table
kable(my.summary, caption = "Summary Statistics of Cytological Characteristics") %>%
  kable_classic_2(full_width = F, latex_options = "HOLD_position")
```

Table 1 shows us that the mean is notably higher than the median for CS, CSh, BN and NN, suggesting that characteristics of malignant tissue are skewing the mean. The standard deviation is also quite large. The table below investigates these findings in more detail with separate summary statistics for benign and malignant tissue.

```{r summary_MB, echo=FALSE}
# Create table
M_B <- tribble(~Variable, ~"Mean", ~"SD", ~"Median", ~"Mean", ~"SD", ~"Median", 
               "CT", "2.96", "1.67", "3.00", "7.19", "2.44",  "8.00",
               "CS", "1,31", "0.86", "1.00", "6.58", "2.72", "6.00",
               "CSh", "1.14", "0.96", "1.00", "6.56", "2.57", "6.00",
               "MA", "1.35", "0.92", "1.00", "5.59", "3.20", "5.00",
               "E", "2.11", "0.88", "2.00", "5.33", "2.44", "5.00",
               "BN", "1.35", "1.18", "1.00", "7.63", "3.12", "10.00",
               "BC", "2.08", "1.06", "2.00", "5.97", "2.28", "7.00",
               "NN", "1.26", "0.95", "1.00", "5.86", "3.35", "6.00", 
               "M", "1.07", "0.51", "1.00", "2.54", "2.40", "1.00")

# Improve visuals of table
kable(M_B, caption = "Summary Statistics by Class",  align = "c") %>%
  add_header_above(c(" " = 1, "Benign" = 3, "Malignant" = 3)) %>%
  kable_styling(full_width = F, latex_options = "HOLD_position")

```

Table 2 clearly demonstrates that there is a significant difference between the characteristics of benign and malignant tissue. This is especially true for CT, BN and BC, indicating they may be the most important predictor variables. 

### Subset Selection

The exploratory data analysis suggests that some predictor variables are likely to be better at predicting our response variable than others. If this proves to be correct, this will allow us to learn the effects of fewer predictor variables more precisely. 

As our response variable is binary (benign or malignant), apply logistic regression is an appropriate approach to identify the best subset of predictor variables. After fitting the logistic regression model for `Class` to standardised data it is clear that CT, BN, MA and BC have a coefficient which is significantly different to zero when testing at the 5% level.

```{r standardise, include=FALSE}
# Standardise the predictor variables 
xs <- scale(x)

# Sample means
center <- attr(xs, "scaled:center")

# Sample standard deviations
scale <- attr(xs, "scaled:scale")

# Reform the data frame
bc_data <- data.frame(xs, Class)
```

```{r logfit, include=FALSE}
# Store n and p
n <- nrow(bc_data)
p <- ncol(bc_data)- 1

# Fit logistic regression model
(logr_fit = glm(Class ~ ., data = bc_data, family="binomial"))

# Summarise the model fit
summary(logr_fit) 

```

To confirm how many predictor variables would be the best subset selection, the Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) are applied. The results are presented in the two plots below.

```{r bss, echo=FALSE, message=FALSE, warning=FALSE}
# Apply best subset selection
fit_AIC <- bestglm(bc_data, family = binomial, IC="AIC")
fit_BIC <- bestglm(bc_data, family = binomial, IC="BIC")

# Identify best-fitting models
best_AIC = fit_AIC$ModelReport$Bestk
best_BIC = fit_BIC$ModelReport$Bestk

# Create a multi-panel plotting device
par(mfrow=c(1,2))

# Produce plots, highlighting optimal value of 'k'
plot(0:p, fit_AIC$Subsets$AIC, xlab = "Number of Predictors", ylab = "AIC", type = "b", main = "Plot 3: AIC Selection", cex.main=0.8)
points(best_AIC, fit_AIC$Subsets$AIC[best_AIC+1], col="#1f78b4", pch=16)

plot(0:p, fit_BIC$Subsets$BIC, xlab = "Number of Predictors", ylab = "BIC", type = "b", main="Plot 4: BIC Selection", cex.main=0.8)
points(best_BIC, fit_BIC$Subsets$BIC[best_BIC+1], col="#1f78b4", pch=16)
```

In both plots, the blue dot is number of predictor variables identified by the best subset selection approach. It appears that a model with six variables is likely to be a good compromise between five and seven. The cytological characteristics for these six variables are CT, CSh, MA, BN, BC, and NN. These align closely to the strongly correlated variables identified in our correlation matrix above.

```{r six, include=FALSE}
# It seems like the model with 6 predictors is a good compromise between 5 and 7 
pstar = 6

# Check which predictors are in the 6-predictor model
fit_AIC$Subsets[pstar+1,]
```

### Modelling

To increase the reliability of our approach, the data is divided in two to allow for out-of-sample validation. In this project, 80% of the data is randomly allocated to a training data set to construct our classifier, with the remaining 20% becoming the testing data set to compute our test errors. 

```{r train, include=FALSE}
# Construct a reduced data set containing only the selected predictors
(indices <- as.logical(fit_AIC$Subsets[pstar+1, 2:(p+1)]))
bc_data_red = data.frame(bc_data[,indices])

# Create test and train data sets
set.seed(683) 
train_index <- sample(nrow(bc_data_red), size = round(0.80 * nrow(bc_data_red)), replace = FALSE)
train <- bc_data_red[train_index,]
test <- bc_data_red[-train_index,]

# Obtain regression coefficients for this model
logr1_fit = glm(Class ~ ., data = train, family="binomial")
summary(logr1_fit)  # Estimate std. are the maximum liklihood estimates of the regression coefficients. Because Cl. thickness and bare.nuclei are the largest positive values, this indicates that leisons with higher numbers in theres are more likely to have malignant cancer?
summ(logr1_fit, scale = TRUE) # Presents details of model fit
summ(logr1_fit, confint = TRUE, digits = 3) # Presents confidence intervals
```
*Logistic Regression*

Logistic regression is used to assign observations to discrete response variables. When applied to our complete data set of six predictor variables, the maximum likelihood estimates of the regression coefficients are presented on the next page.

```{r log_coefficients, echo=FALSE}
# Create table
logr_coefficients <- tribble(~"", ~"Intercept", ~"CT", ~"CSh", ~"MA", ~"BN", ~"BC", ~"NN", "Estimates", -1.28, 2.08, 1.14, 1.39, 1.69, 1.52, 0.87)

# Improve visuals
kable(logr_coefficients, caption = "Estimates of Regression Coefficients for Logistic Regression") %>%
  kable_classic_2(full_width = F, latex_options = "HOLD_position")
```

```{r log_error, include = FALSE}
# Training error
p1 <- predict(logr1_fit, train, type="response") 
(y = as.numeric(ifelse(p1 > 0.5, 1, 0))) # Would be likely to have malignant 

# Calculate fitted values 
phat <- predict(logr1_fit, train, type = "response") # Compute predicted probabilities
yhat <- ifelse(phat > 0.5, 1, 0) # Compute predicted values

#Compute the confusion matrix
(confusion <- table(Observed=train$Class, predicted=yhat))

# Normalise function
normalise = function(x) {
  return(x / sum(x))
}

# Apply function to the confusion matrix
t(apply(confusion, 1, normalise)) # Performance is okay.# Perform prediction 2

# Calculate the training error
1 - sum(diag(confusion)) / sum(confusion) # 0.02014652 (2.02%)

# Calculate Test error
logr_test = predict(logr1_fit, test, type ="response") 
yhat_logr_test = ifelse(logr_test > 0.5, 1, 0)

#Compute the confusion matrix
(confusion_2 <- table(Observed=test$Class, predicted=yhat_logr_test))

# Apply function to the confusion matrix
t(apply(confusion_2, 1, normalise)) # Performance is okay.

# Compute test error
(1-mean(test$Class == yhat_logr_test)) #0.05847953 (5.84%)

#Compute the confusion matrix
(confusion_2 <- table(Observed=test$Class, predicted=yhat_logr_test))

# Apply function to the confusion matrix
t(apply(confusion_2, 1, normalise)) # Performance is okay.
```

With 'in-sample validation', the training error is 2.02%. This is not very interesting as only the test error measures how well the method performs on previously unseen data (@Project). When using our train and test data sets for out-of-sample validation, the test error is 5.85%. This is slightly larger than the training error, which may indicate that necessary predictor variables have been included. (IF you HAVE TIME RE-RUN EXCLUDING CELL-SHAPE which has the largest p-variable)

The accuracy of our model is visualised by the Receiver Operating Characteristic (ROC) curve below. 

```{r log_roc, echo=FALSE, message=FALSE, warning=FALSE}
# ROC plot of logistic regression
logr_test_roc <- roc(test$Class ~ logr_test, plot = TRUE, print.auc = TRUE, main="Plot 5: Logistic Regression ROC", col="#1f78b4")
```

The measure of accuracy is the area under the ROC curve. This curve is far from the diagonal and is quite close to the perfect accuracy area of 1.00. Helpfully, the plot specifies the accuracy of our model as 0.99.

*LASSO Regression*

Regularisation methods, such as LASSO, are shrinkage methods; they work to minimise the loss function and shrink the maximum likelihood estimates of regression coefficients to zero. LASSO performs subset selection in addition to shrinkage; given that the test error was higher than the training error for logistic regression, this method is selected over ridge regression. The LASSO regression applied to the same training data as above is visualised below.

```{r LASSO, echo=FALSE}
# Choose grid of values for the turning parameter
grid = 10^seq(5, -3, length = 100)

#Fit a model with LASSO penalty for each value of the turning parameter
lasso_fit_train = glmnet(train[1:6], train$Class, family = "binomial", alpha = 1, standardize = FALSE, lambda = grid)

## Examine the effect of the tuning parameter on the parameter estimates
plot(lasso_fit_train, xvar="lambda", col=rainbow(p), label=TRUE, main = "Plot 6: LASSO Regression", cex.main=0.8, mar=c(3, 2, 2, 0))
```

The coefficients of the LASSO are best presented graphically; the plot below shows the effect of the turning parameter when applying LASSO to our training data set.

```{r Lasso_coefficients, echo=FALSE}
# Plot coefficients
plot(lasso_fit_train, xvar="lambda", col=1:7, label=TRUE, main = "Plot 7: LASSO Regression Coefficients", cex.main=0.8, mar=c(3, 2, 2, 0))
```

This plot shows us that the third variable, MA (green), is the first to drop out of the model, followed by our 6th variable, NN (magenta), and so on. 

To choose an appropriate value for the turning parameter of LASSO, 10-fold cross-validation is used. This allows the optimal turning parameter and mean MSE to be calculated, which are 0.001 and 0.004222146, respectively. The regression coefficients obtained by performing the LASSO with the optimal turning parameter are shown in the table below.

```{r MSE, include=FALSE}
# Cross-validation
train_l <- as.matrix(train)
lasso_train_cv_fit = cv.glmnet(train_l, train$Class, family = "binomial", alpha = 1, standardize = FALSE, lambda = grid)
plot(lasso_train_cv_fit)

## Identify the optimal value for the tuning parameter
(lambda_lasso_min = lasso_train_cv_fit$lambda.min)
# 0.001
(which_lambda_lasso = which(lasso_train_cv_fit$lambda == lambda_lasso_min))
#100

# Extract corresponding mean MSE
lasso_train_cv_fit$cvm[which_lambda_lasso] #0.004222146

## Find the parameter estimates associated with optimal value of the tuning parameter
coef(lasso_train_cv_fit, s=lambda_lasso_min)

# Create table
lasso_coefficients <- tribble(~"", ~"Intercept", ~"CT", ~"CSh", ~"MA", ~"BN", ~"BC", ~"NN", "Estimates", -5.43, 0.35, 0.52, 0.15, 0.60, 0.25, 0.17)

# Improve visuals
kable(lasso_coefficients, caption = "Estimates of Regression Coefficients for Logistic Regression") %>%
  kable_classic_2(full_width = F, latex_options = "HOLD_position")
```
```{r lasso_test, include=FALSE}
# Calculate Test error
#Fit a model with LASSO penalty for each value of the turning parameter
lasso1_fit_train = glmnet(train, train$Class, family = "binomial", alpha = 1, standardize = FALSE, lambda = grid)
# Compute test error
test_l <- as.matrix(test)
lasso_test = predict(lasso1_fit_train, test_l, type ="response") 
yhat_lasso_test = ifelse(lasso_test > 0.5, 1, 0)

# Compute test error
(1-mean(test$Class == yhat_lasso_test)) #0.2572263 25.72%
```

When using our train and test data sets for out-of-sample validation, the test error is 25.75%, so significantly higher than the logistic regression.

*Discriminant Analysis*



**If you have time, do it the other way round - a model conditional on maliganant and another condition not on malignant**

 In all three cases small values of the information criterion indicate a “better” model. 



– At least one discriminant analyis method, i.e. the Bayes classifier for linear disciminant
analysis (LDA) or quadratic discriminant analysis (QDA). (Practical 6)
(As discussed in the previous section, in LDA we assume that the conditional distributions
for the predictor variables X are multivariate normal, with a group-specific mean vector and
a common covariance matrix. Quadratic discriminant analysis (QDA) is similar except the
different groups are allowed to have different covariance matrices... The details of this estimation procedure for the mean vectors and covariance matrices are
beyond the scope of this course. For the group membership probabilities k, one method of
estimation is to use the sample proportions. - Chapter 5) 


- For the variants of logistic regression, you should present the coefficients of the fitted model,
and any other useful graphical or numerical summaries. For LDA and QDA present estimates
of the group means. In each case, discuss what your results show. For example, which variables
drop out of the model when you use subset selection or the LASSO? What do the parameters
tell you about the relationships between the response and predictor variables?

coefficient of determination (R squred) means that x% of the variation in log PSa can be explained by regression on the eight standardised predictors

Performance %>%
  ggplot(aes(x = av_mem, y = av_util, color = CoV_m)) + geom_point(position = "jitter") + geom_smooth(color = "#525252") + labs(x = "Average Memory (percentage)", y = "Average Utilisation (percentage)", title = "Plot 11: Average Utilisation by Average Memory Usage per Hostname", color = "Memory Variation") # Very clear trajectory of more memory corresponds to utilising  more gpu core

• Compare the performance of your models using cross-validation based on the test error. Think
about how you might do this in a way that makes the comparison fair.


(We typically call the data used to construct the classifier
our training data. The data used to estimate the pij , and hence “validate” the classifier,
are called the validation data. If the same data are used to both train and validate the
classifier, this is called in-sample validation. If the training and validation data sets are
different, this is called out-of-sample validation... The K×K matrix of counts nij is often called the confusion matrix.
Although the empirical method is simple and widely used, it also leads to optimistic estimates
of the performance of the classification scheme due to overfitting. Again this is essentially
caused by double-use of the data for constructing and testing the classifier.
Often, especially when comparing classification schemes, it is useful to characterise the overall
quality of the scheme using a single number summary... The training error rate
...Clearly,
in the case of a perfect classifier the error rate would be 0%. We generally prefer classifiers
with a low error rate. - CHapter 5) Likely to be using in-sample? Out sample mor rigerus - it is possible, but will need to change training data

• Select a final “best” classifier, justifying your choice. Does it include all the predictor variables?
Why or why not? (page 120? in notes of chapter 5)
assessed on thier predictie performance using techniques such as cross validation.
"what is the probability that an individual has the disease if they have clinical measurements of x?"

\newpage
### Appendix A: Denisty Distributions of Cytological Characteristics by Class
```{r density, echo=FALSE, fig.height=11.5, fig.width=9}
# Density Distribution of Clump Thickness
CT <- BC1 %>%
  ggplot(aes(x = Cl.thickness, colour = Class)) + geom_density(kernel = "gaussian", size = 1.5) + labs(x = "Clump Thickness", y = "Density", legend = "Class") + scale_color_brewer(palette = "Paired") + theme(axis.title.y=element_blank())

# Density Distribution of Cell Size
CS <- BC1 %>%
  ggplot(aes(x = Cell.size, colour = Class)) + geom_density(kernel = "gaussian", size = 1.5) + labs(x = "Cell Size", y = "Density", legend = "Class") + scale_color_brewer(palette = "Paired") + theme(axis.title.y=element_blank())

# Density Distribution
CSh <- BC1 %>%
  ggplot(aes(x = Cell.shape, colour = Class)) + geom_density(kernel = "gaussian", size = 1.5) + labs(x = "Cell shape", y = "Density", legend = "Class") + scale_color_brewer(palette = "Paired") + theme(axis.title.y=element_blank())

# Density Distribution of Marginal Adhesion
MA <- BC1 %>%
  ggplot(aes(x = Marg.adhesion, colour = Class)) + geom_density(kernel = "gaussian", size = 1.5) + labs(x = "Marginal Adhesion", y = "Density", legend = "Class") + scale_color_brewer(palette = "Paired") + theme(axis.title.y=element_blank())

# Density Distribution of Single Epithelial Cell Size
E <- BC1 %>%
  ggplot(aes(x = Epith.c.size, colour = Class)) + geom_density(kernel = "gaussian", size = 1.5) + labs(x = "Single Epithelial Cell Size", y = "Density", legend = "Class") + scale_color_brewer(palette = "Paired") + theme(axis.title.y=element_blank())

# Density Distribution of Bare Nuclei
BN <- BC1 %>%
  ggplot(aes(x = Bare.nuclei, colour = Class)) + geom_density(kernel = "gaussian", size = 1.5) + labs(x = "Bare Nuclei", y = "Density", legend = "Class") + scale_color_brewer(palette = "Paired") + theme(axis.title.y=element_blank())

# Density Distribution of Bland Chromatin
BC <- BC1 %>%
  ggplot(aes(x = Bl.cromatin, colour = Class)) + geom_density(kernel = "gaussian", size = 1.5) + labs(x = "Bland Chromatin", y = "Density", legend = "Class") + scale_color_brewer(palette = "Paired") + theme(axis.title.y=element_blank())

# Density Distribution of Normal Nucleoli
NN <- BC1 %>%
  ggplot(aes(x = Normal.nucleoli, colour = Class)) + geom_density(kernel = "gaussian", size = 1.5) + labs(x = "Normal Nucleoli", y = "Density", legend = "Class") + scale_color_brewer(palette = "Paired") + theme(axis.title.y=element_blank())

# Density Distribution of Mitoses
M <- BC1 %>%
  ggplot(aes(x = Mitoses, colour = Class)) + geom_density(kernel = "gaussian", size = 1.5) + labs(x = "Mitoses", y = "Density", legend = "Class") + scale_color_brewer(palette = "Paired") + theme(axis.title.y=element_blank())

# Visualise plots as grid. 
TQ <- ggarrange(CT, CS, CSh, MA, E, BN, BC, NN, M, ncol=3, nrow=3, common.legend = TRUE, legend="top")
annotate_figure(TQ, left = text_grob("Density", rot = 90, vjust = 1))
```

\newpage
### Bibliography