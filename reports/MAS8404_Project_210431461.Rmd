---
title: "MAS8404_Project_210431461"
author: "210431461  |   21/10/22"
output: pdf_document
fontsize: 12pt
citation_package: natbib
bibliography: "references.bib"
biblio-style: "apalike"
link-citations: TRUE
---

\vspace{-0.7cm}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir= normalizePath('..'))
options(tinytex.verbose = TRUE)
```

```{r ProjectTemplate, include = FALSE}
library(ProjectTemplate)
load.project()
```

### Introduction 

At the University of Wisconsin Hospital, Dr. Wolberg (@Wolberg) collected breast tissue samples from women using fine needle aspiration cytology (FNAC)(@Project). Histological examination of the tissue collected by this procedure allows for a physician to determine whether or not the tissue is benign or malignant. Our objective is to build a classifier that determines whether a tissue sample is likely to be benign or malignant based on its cytological characteristics.

### Data Description

Dr. Wolberg reported his data chronologically to the `mlbench` R package (@Wolberg). The FNAC procedure allows for the identification of nine cytological characteristics (@Project): clump thickness (Cl.thickness / CT), uniformity of cell size (Cell.size / CS), uniformity of cell shape (Cell.shape / CSh), marginal adhesion (Marg.adhesion / MA), single epithelial cell size (Epith.c.size / E), bare nuclei (Bare.nuclei / BN), bland chromatin (Bl.chromatin / BC), normal nucleoli (Normal.nucleoli / NN) and mitoses (Mitoses / M). These characteristics for each tissue sample are measured on a discrete scale of one to ten, where smaller numbers indicate that the sample is healthier (@Project). To aid our analysis, the ordinal variables of this scale have been converted to quantitative variables.

This report explores data from a sample of 699 women in the `BreastCancer` data set; please note that 16 of the 699 observations have been removed due to missing attribute values. It is assumed that this is a random sample of women experiencing symptoms of breast cancer (@Project). Each woman is represented by a sample code number (Id) that reflects the chronological grouping of this data (@Wolberg). The data set also includes the result of further histological examination (Class), which confirms whether each woman's tissue sample was begin or malignant; this has been converted into a binary variable of 0 or 1, respectively.

### Data Exploration

Of the tissues samples from 683 women, 239 are confirmed as malignant; in our sample, 53.83% of the women who are experiencing breast cancer symptoms have malignant tissue. Whether the tissue is benign or malignant is considered the response variable, and the cytological characteristics of tissue are considered the predictor variables. The characteristics of malignant tissue tend to have higher numbers on the scale one to ten; this is visualized by the density distributions provided in `Appendix A`. All nine cytological characteristics of the tissue samples are presented below in a scatter plot matrix, where blue indicates the tissue is malignant.

```{r summmary_plot, echo=FALSE, fig.width=9, fig.height=9, fig.align='center'}
# Convert data into numeric matrix
BC2 <- data.matrix(BC1) 

# Extract the response variable
Class <- BC2[,11]-1

#Extract the predictor variables
x <- BC2[,2:10] 

# Quick pairs plot
pairs(x, col=c("black", "#1f78b4")[Class+1], main="Plot 1: Scatter Plot Matrix of All Cytological Characteristics")
```

This plot visualises the relationship between the characteristics; it suggests that there is a strong linear relationship between CS and CSh, indicating that larger cells have a more significant shape and more are likely to be malignant. There also appears to be a weak linear relationship between CS and CSh with E and BC, as well as a weak linear relationship between CT and CSh. Further investigation of these relationships would be beneficial. 

The correlations between other characteristics are not initially obvious and require deeper analysis. A sample correlation matrix quantifies the strength of the linear relationship and, for all characteristics, is visualised by the correlation plot below.

```{r summmary_cor, echo=FALSE, fig.width=5, fig.height=5, fig.align='center'}
# Observe the correlations
cor_bc <- cor(BC1[,2:10])

# Plot correlations
corrplot(cor_bc, order = "hclust", method = "circle", tl.col="black", tl.cex = 0.7, main="Plot 2: Cytological Characteristics Correlation", cex.main=0.8, mar=c(3, 2, 2, 0)) 
```

\vspace{-1.5cm}

In this plot, the correlation is more significant when the circle is both darker and larger. This plot confirms the strong linear relationship between CS and CSh, which suggests that any classifier is unlikely to need both characteristics. These characteristics also appear to have stronger relationships with CT, MA, E, BN, BC, and NN. M does not appear to have a significant correlation with any of the characteristics. These insights indicate that an accurate classifier may only need a selection of the predictor variables. 

```{r general_variance, include=FALSE}
# Sample variance of data
s <- var(BC2)

# Generalised variance 
det(s) #55382860
```

```{r total_variation, include=FALSE}
# Sample variance of data
s <- var(BC2)

# Calculate total variation
s_sq = diag(s) # Extract diagonal elements

(total_variation = sum(s_sq))
#33283.24
```

Boxplots of each cytological characteristic are provided in `Appendix B`; these highlight that there is a distinctive difference between the characteristics of benign and malignant tissue. The mean, median and standard deviation (SD) of our data is, therefore, likely to be skewed unless the data is filtered by its Class (benign or malignant). The filtered summary statistics are presented in Table 1 on the next page.

```{r summary, include=FALSE}
# Create table
my.summary <- tribble(~"Variable", ~"CT", ~"CS", ~"CSh", ~"MA", ~"E", ~"BN", ~"BC", ~"NN", ~"M", "Median", 4.00, 1.00, 1.00, 1.00, 2.00, 1.00, 3.00, 1.00, 1.00, "Mean", 4.44, 3.15, 3.22, 2.83, 3.23, 2.54, 3.45, 2.87, 1.58, "SD", 2.82, 3.07, 2.99, 2.86, 2.22, 3.64, 2.45, 3.05, 1.64)

# Improve visuals of table
kable(my.summary, caption = "Summary Statistics of Cytological Characteristics") %>%
  kable_classic_2(full_width = F, latex_options = "HOLD_position")

#This table shows us that the mean is notably higher than the median for CS, CSh, BN and NN, suggesting that characteristics of malignant tissue are skewing the mean. The standard deviation is also quite large. 
```

```{r summary_MB, echo=FALSE}
# Create table
M_B <- tribble(~Variable, ~"Mean", ~"SD", ~"Median", ~"Mean", ~"SD", ~"Median", 
               "CT", "2.96", "1.67", "3.00", "7.19", "2.44",  "8.00",
               "CS", "1,31", "0.86", "1.00", "6.58", "2.72", "6.00",
               "CSh", "1.14", "0.96", "1.00", "6.56", "2.57", "6.00",
               "MA", "1.35", "0.92", "1.00", "5.59", "3.20", "5.00",
               "E", "2.11", "0.88", "2.00", "5.33", "2.44", "5.00",
               "BN", "1.35", "1.18", "1.00", "7.63", "3.12", "10.00",
               "BC", "2.08", "1.06", "2.00", "5.97", "2.28", "7.00",
               "NN", "1.26", "0.95", "1.00", "5.86", "3.35", "6.00", 
               "M", "1.07", "0.51", "1.00", "2.54", "2.40", "1.00")

# Improve visuals of table
kable(M_B, caption = "Summary Statistics by Class",  align = "c") %>%
  add_header_above(c(" " = 1, "Benign" = 3, "Malignant" = 3)) %>%
  kable_styling(full_width = F, latex_options = "HOLD_position")

```

This table clearly demonstrates that there is a significant difference between the characteristics of benign and malignant tissue. This is especially true for CT, BN and BC, which suggests that they may be the most important predictor variables.

There are two single measures of multivariate scatter that help generalise how varied our data is; the generalised variance, which for our data is 55382860.00, and the total variation, which for our data is 33283.24. These large values tell us that there is a high degree of scatter about the sample means of each variable. This highlights that it will be important to transform our data to standardised it so that all the variables are put on a common scale. 

### Subset Selection

The exploratory data analysis suggests that some predictor variables are likely to be better at predicting our response variable than others. If this proves to be correct, this will allow us to learn the effects of fewer predictor variables more precisely. 

As our response variable is binary (benign or malignant), apply logistic regression is an appropriate approach to identify the best subset of predictor variables. After fitting the logistic regression model for `Class` to standardised data it is clear that CT, BN, MA and BC have a coefficient which is significantly different to zero when testing at the 5% level.

```{r standardise, include=FALSE}
# Standardise the predictor variables 
xs <- scale(x)

# Sample means
center <- attr(xs, "scaled:center")

# Sample standard deviations
scale <- attr(xs, "scaled:scale")

# Reform the data frame
bc_data <- data.frame(xs, Class)
```

```{r logfit, include=FALSE}
# Store n and p
n <- nrow(bc_data)
p <- ncol(bc_data)- 1

# Fit logistic regression model
(logr_fit = glm(Class ~ ., data = bc_data, family="binomial"))

# Summarise the model fit
summary(logr_fit) 

```

To confirm how many predictor variables would be the best subset selection, the Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) are applied. The results are presented in the two plots below.

```{r bss, echo=FALSE, message=FALSE, warning=FALSE}
# Apply best subset selection
fit_AIC <- bestglm(bc_data, family = binomial, IC="AIC")
fit_BIC <- bestglm(bc_data, family = binomial, IC="BIC")

# Identify best-fitting models
best_AIC = fit_AIC$ModelReport$Bestk
best_BIC = fit_BIC$ModelReport$Bestk

# Create a multi-panel plotting device
par(mfrow=c(1,2))

# Produce plots, highlighting optimal value of 'k'
plot(0:p, fit_AIC$Subsets$AIC, xlab = "Number of Predictors", ylab = "AIC", type = "b", main = "Plot 3: AIC Selection", cex.main=0.8)
points(best_AIC, fit_AIC$Subsets$AIC[best_AIC+1], col="#1f78b4", pch=16)

plot(0:p, fit_BIC$Subsets$BIC, xlab = "Number of Predictors", ylab = "BIC", type = "b", main="Plot 4: BIC Selection", cex.main=0.8)
points(best_BIC, fit_BIC$Subsets$BIC[best_BIC+1], col="#1f78b4", pch=16)
```

In both plots, the blue dot is number of predictor variables identified by the best subset selection approach. It appears that a model with six variables is likely to be a good compromise between five and seven. The cytological characteristics for these six variables are CT, CSh, MA, BN, BC, and NN. These align closely to the strongly correlated variables identified in our correlation matrix above.

```{r six, include=FALSE}
# It seems like the model with 6 predictors is a good compromise between 5 and 7 
pstar = 6

# Check which predictors are in the 6-predictor model
fit_AIC$Subsets[pstar+1,]
```

### Modelling

To increase the reliability of our approach, the data is divided in two to allow for out-of-sample validation. In this project, 80% of the data is randomly allocated to a training data set to construct our classifier, with the remaining 20% becoming the testing data set to compute our test errors. 

```{r train, include=FALSE}
# Construct a reduced data set containing only the selected predictors
(indices <- as.logical(fit_AIC$Subsets[pstar+1, 2:(p+1)]))
bc_data_red = data.frame(bc_data[,indices])

# Create test and train data sets
set.seed(683) 
train_index <- sample(nrow(bc_data_red), size = round(0.80 * nrow(bc_data_red)), replace = FALSE)
train <- bc_data_red[train_index,]
test <- bc_data_red[-train_index,]

# Obtain regression coefficients for this model
logr1_fit = glm(Class ~ ., data = train, family="binomial")
summary(logr1_fit)  # Estimate std. are the maximum liklihood estimates of the regression coefficients. Because Cl. thickness and bare.nuclei are the largest positive values, this indicates that leisons with higher numbers in theres are more likely to have malignant cancer?
summ(logr1_fit, scale = TRUE) # Presents details of model fit
summ(logr1_fit, confint = TRUE, digits = 3) # Presents confidence intervals
```
*Logistic Regression*

Logistic regression is used to assign observations to discrete response variables. When applied to our complete data set of six predictor variables, the maximum likelihood estimates of the regression coefficients are presented on the next page.

```{r log_coefficients, echo=FALSE}
# Create table
logr_coefficients <- tribble(~"", ~"Intercept", ~"CT", ~"CSh", ~"MA", ~"BN", ~"BC", ~"NN", "Estimates", -1.28, 2.08, 1.14, 1.39, 1.69, 1.52, 0.87)

# Improve visuals
kable(logr_coefficients, caption = "Estimates of Regression Coefficients for Logistic Regression") %>%
  kable_classic_2(full_width = F, latex_options = "HOLD_position")
```

```{r log_error, include = FALSE}
# Training error
p1 <- predict(logr1_fit, train, type="response") 
(y = as.numeric(ifelse(p1 > 0.5, 1, 0))) # Would be likely to have malignant 

# Calculate fitted values 
phat <- predict(logr1_fit, train, type = "response") # Compute predicted probabilities
yhat <- ifelse(phat > 0.5, 1, 0) # Compute predicted values

#Compute the confusion matrix
(confusion <- table(Observed=train$Class, predicted=yhat))

# Normalise function
normalise = function(x) {
  return(x / sum(x))
}

# Apply function to the confusion matrix
t(apply(confusion, 1, normalise)) # Performance is okay.# Perform prediction 2

# Calculate the training error
1 - sum(diag(confusion)) / sum(confusion) # 0.02014652 (2.02%)

# Calculate Test error
logr_test = predict(logr1_fit, test, type ="response") 
yhat_logr_test = ifelse(logr_test > 0.5, 1, 0)

#Compute the confusion matrix
(confusion_2 <- table(Observed=test$Class, predicted=yhat_logr_test))

# Apply function to the confusion matrix
t(apply(confusion_2, 1, normalise)) # Performance is okay.

# Compute test error
(1-mean(test$Class == yhat_logr_test)) #0.05847953 (5.84%)

#Compute the confusion matrix
(confusion_2 <- table(Observed=test$Class, predicted=yhat_logr_test))

# Apply function to the confusion matrix
t(apply(confusion_2, 1, normalise)) # Performance is okay.
```

With 'in-sample validation', the training error is 2.02%. This is not very interesting as only the test error measures how well the method performs on previously unseen data (@Project). When using our train and test data sets for out-of-sample validation, the test error is 5.85%. This is slightly larger than the training error, which may indicate that unnecessary predictor variables have been included. 

The accuracy of our model is visualised by the Receiver Operating Characteristic (ROC) curve below. 

```{r log_roc, echo=FALSE, message=FALSE, warning=FALSE}
# ROC plot of logistic regression
logr_test_roc <- roc(test$Class ~ logr_test, plot = TRUE, print.auc = TRUE, main="Plot 5: Logistic Regression ROC", cex.main=0.8, col="#1f78b4")
```

The measure of accuracy is the area under the ROC curve. This curve is far from the diagonal and is quite close to the perfect accuracy area of 1.00. Helpfully, the plot specifies the accuracy of our model as 0.99.

*LASSO Regression*

Regularisation methods, such as LASSO, are shrinkage methods; they work to minimise the loss function and shrink the maximum likelihood estimates of regression coefficients to zero. LASSO performs subset selection in addition to shrinkage; given that the test error was higher than the training error for logistic regression, this method is selected over ridge regression. The LASSO regression applied to the same training data as above is visualised below.

```{r LASSO, echo=FALSE}
# Choose grid of values for the turning parameter
grid = 10^seq(5, -3, length = 100)

#Fit a model with LASSO penalty for each value of the turning parameter
lasso_fit_train = glmnet(train[,1:6], train$Class, family = "binomial", alpha = 1, standardize = FALSE, lambda = grid)

## Examine the effect of the tuning parameter on the parameter estimates
plot(lasso_fit_train, xvar="lambda", col=rainbow(p), label=TRUE, main = "Plot 6: LASSO Regression", cex.main=0.8, mar=c(3, 2, 2, 0))
```

This plot presents the coefficients of the LASSO regression and the effect of the turning parameter when applying LASSO to our training data set. It shows us that the third variable, MA (green), is the first to drop out of the model, followed by our 6th variable, NN (blue), and so on. 

To choose an appropriate value for the turning parameter of LASSO, 10-fold cross-validation is used. This allows the optimal turning parameter and mean MSE to be calculated, which are 0.001 and 0.004222146, respectively. The regression coefficients obtained by performing the LASSO with the optimal turning parameter are shown in the table below.

```{r MSE, include=FALSE}
# Cross-validation
train_l <- as.matrix(train)
lasso_train_cv_fit = cv.glmnet(train_l, train$Class, family = "binomial", alpha = 1, standardize = FALSE, lambda = grid)
plot(lasso_train_cv_fit)

## Identify the optimal value for the tuning parameter
(lambda_lasso_min = lasso_train_cv_fit$lambda.min)
# 0.001
(which_lambda_lasso = which(lasso_train_cv_fit$lambda == lambda_lasso_min))
#100

# Extract corresponding mean MSE
lasso_train_cv_fit$cvm[which_lambda_lasso] #0.004222146

## Find the parameter estimates associated with optimal value of the tuning parameter
coef(lasso_train_cv_fit, s=lambda_lasso_min)

# Create table
lasso_coefficients <- tribble(~"", ~"Intercept", ~"CT", ~"CSh", ~"MA", ~"BN", ~"BC", ~"NN", "Estimates", -5.43, 0.35, 0.52, 0.15, 0.60, 0.25, 0.17)

# Improve visuals
kable(lasso_coefficients, caption = "Estimates of Regression Coefficients for Logistic Regression") %>%
  kable_classic_2(full_width = F, latex_options = "HOLD_position")
```
```{r lasso_test, include=FALSE}
# Calculate Test error
#Fit a model with LASSO penalty for each value of the turning parameter
lasso1_fit_train = glmnet(train, train$Class, family = "binomial", alpha = 1, standardize = FALSE, lambda = grid)
# Compute test error
test_l <- as.matrix(test)
lasso_test = predict(lasso1_fit_train, test_l, type ="response") 
yhat_lasso_test = ifelse(lasso_test > 0.5, 1, 0)

# Compute test error
(1-mean(test$Class == yhat_lasso_test)) #0.2572263 25.72%
```

When using our train and test data sets for out-of-sample validation, the test error is 25.75%, so significantly higher than the logistic regression.

*Discriminant Analysis*

Discriminant analysis is a technique used to classify observations into groups that do not overlap. Linear Discriminant Analysis (LDA) assumes that the common covariance matrix of the predictor variables is the same. Quadratic Discriminant Analysis (QDA) does not assume this. However, in practice, whether the common covariance is generally unknown. 

The density distributions in `Appendix A` and boxplots in `Appendix B` suggest that our data is not normally distributed and indicates non-equal variances (respectively). QDA is likely to be more appropriate than LDA. The group means generated by both LDA and QDA are the same, and so only one version of the group means is presented in the table on the next page.

```{r groupmeans, echo=FALSE}
# Perform LDA 
lda_train <- lda(Class~., data=train, type = "response")

# Perform QDA
qda_train <- qda(Class~., data=train)

# Create table
DA <- tribble(~"", ~"CT", ~"CSh", ~"MA", ~"BN", ~"BC", ~"NN",
              "Benign", -0.56, -0.61, -0.53, -0.61, -0.57, -0.53, 
              "Malignant", 0.95, 1.13, 0.96, 1.15, 1.03, 0.99) 

# Improve visuals of table
kable(DA, caption = "LDA and QDA Group Means",  align = "c") %>%
  kable_styling(full_width = F, latex_options = "HOLD_position")
```

```{r DA_test_error, include=FALSE}
## Compute fitted values for the lda validation data
lda_test = predict(lda_train, test)
lda_yhat_test = lda_test$class 

## Compute test error
1 - mean(test$Class == lda_yhat_test) # 0.0729927, 7.30%

## Compute fitted values for the qda validation data
qda_test = predict(qda_train, test, type = "response")
yhat_test = qda_test$class

## Compute test error
1 - mean(test$Class == yhat_test) # 0.06569343, 6.57%
```

When using our train and test data sets for out-of-sample validation, the test error for LDA is 7.30% and QDA is 6.57%. As expected QDA has the lower test error. 

### Evaluation

Based on the predictive performance of the three models applied, the logistic regression has the lowest test error at 5.85%. It if followed by QDA (6.57%), LDA (7.30%) and, lastly, the LASSO regression with a test error of 25.75%. I had hoped to be able to compare all the models with ROC plots, and unfortunately there was an issue with the code for LASSO and the decriminant analyses. Smaller test errors indicate more accurate models, so our logistic regression would be the best classifier. 

Our best classifier does not include all nine cytological characteristics. Using the methods of AIC and BIC for best subset selection, it was determined that six predictor variables was likely to provide the most accurate model. There were: CT, CSh, MA, BN, BC, and NN.

However, as mentioned in the logistic regression section, the test error was slightly larger than the training error. This indicates unnecessary predictor variables have been included. Cell Shape (CSh) had the largest p-value of the chosen six variables, so the classifier may benefit from re-running the logistic regression without this variable.

Similarly, it was interesting that the LASSO regression did conduct any further subset selection from the six variables. It would be interesting to re-run the LASSO regression on the original standardised data, before best subset selection with the logistic regression, to determine whether this would improve the accuracy of the model.

All of the models applied in this report have test error rates that would be impractical in a clinical setting. Further investigation would be beneficial to improve the accuracy of the models; this could include different methods of best subset selection, dividing the test and train data differently (perhaps by only using cross-validation k-folds), and, ideally, access to a much larger data set to avoid over-fitting the models. 

\newpage
### Appendix A: Denisty Distributions of Cytological Characteristics by Class
```{r density, echo=FALSE, fig.height=11.5, fig.width=9, message=FALSE, warning=FALSE}
# Density Distribution of Clump Thickness
CT <- BC1 %>%
  ggplot(aes(x = Cl.thickness, colour = Class)) + geom_density(kernel = "gaussian", size = 1.5) + labs(x = "Clump Thickness", y = "Density", legend = "Class") + scale_color_brewer(palette = "Paired") + theme(axis.title.y=element_blank())

# Density Distribution of Cell Size
CS <- BC1 %>%
  ggplot(aes(x = Cell.size, colour = Class)) + geom_density(kernel = "gaussian", size = 1.5) + labs(x = "Cell Size", y = "Density", legend = "Class") + scale_color_brewer(palette = "Paired") + theme(axis.title.y=element_blank())

# Density Distribution
CSh <- BC1 %>%
  ggplot(aes(x = Cell.shape, colour = Class)) + geom_density(kernel = "gaussian", size = 1.5) + labs(x = "Cell shape", y = "Density", legend = "Class") + scale_color_brewer(palette = "Paired") + theme(axis.title.y=element_blank())

# Density Distribution of Marginal Adhesion
MA <- BC1 %>%
  ggplot(aes(x = Marg.adhesion, colour = Class)) + geom_density(kernel = "gaussian", size = 1.5) + labs(x = "Marginal Adhesion", y = "Density", legend = "Class") + scale_color_brewer(palette = "Paired") + theme(axis.title.y=element_blank())

# Density Distribution of Single Epithelial Cell Size
E <- BC1 %>%
  ggplot(aes(x = Epith.c.size, colour = Class)) + geom_density(kernel = "gaussian", size = 1.5) + labs(x = "Single Epithelial Cell Size", y = "Density", legend = "Class") + scale_color_brewer(palette = "Paired") + theme(axis.title.y=element_blank())

# Density Distribution of Bare Nuclei
BN <- BC1 %>%
  ggplot(aes(x = Bare.nuclei, colour = Class)) + geom_density(kernel = "gaussian", size = 1.5) + labs(x = "Bare Nuclei", y = "Density", legend = "Class") + scale_color_brewer(palette = "Paired") + theme(axis.title.y=element_blank())

# Density Distribution of Bland Chromatin
BC <- BC1 %>%
  ggplot(aes(x = Bl.cromatin, colour = Class)) + geom_density(kernel = "gaussian", size = 1.5) + labs(x = "Bland Chromatin", y = "Density", legend = "Class") + scale_color_brewer(palette = "Paired") + theme(axis.title.y=element_blank())

# Density Distribution of Normal Nucleoli
NN <- BC1 %>%
  ggplot(aes(x = Normal.nucleoli, colour = Class)) + geom_density(kernel = "gaussian", size = 1.5) + labs(x = "Normal Nucleoli", y = "Density", legend = "Class") + scale_color_brewer(palette = "Paired") + theme(axis.title.y=element_blank())

# Density Distribution of Mitoses
M <- BC1 %>%
  ggplot(aes(x = Mitoses, colour = Class)) + geom_density(kernel = "gaussian", size = 1.5) + labs(x = "Mitoses", y = "Density", legend = "Class") + scale_color_brewer(palette = "Paired") + theme(axis.title.y=element_blank())

# Visualise plots as grid. 
TQ <- ggarrange(CT, CS, CSh, MA, E, BN, BC, NN, M, ncol=3, nrow=3, common.legend = TRUE, legend="top")
annotate_figure(TQ, left = text_grob("Density", rot = 90, vjust = 1))
```

\newpage
### Appendix B: Boxplots of Cytological Characteristics by Class
```{r boxplots, echo=FALSE, fig.height=11.5, fig.width=9}

B_CT <- ggplot(BC1, aes(x = Class, y = Cl.thickness)) + geom_boxplot(alpha = 0.3, color = "#1f78b4") + labs(x = "Class", y = "Clump Thickness (scale)", title = "Clump Thickness by Class") + theme(legend.position = "none") + expand_limits(y = 0)

B_CS <- ggplot(BC1, aes(x = Class, y = Cell.size)) + geom_boxplot(alpha = 0.3, color = "#1f78b4") + labs(x = "Class", y = "Cell Size (scale)", title = "Cell Size by Class") + theme(legend.position = "none") + expand_limits(y = 0)

B_CSh <- ggplot(BC1, aes(x = Class, y = Cell.shape)) + geom_boxplot(alpha = 0.3, color = "#1f78b4") + labs(x = "Class", y = "Cell Shape (scale)", title = "Cell Shape by Class") + theme(legend.position = "none") + expand_limits(y = 0)

B_MA <- ggplot(BC1, aes(x = Class, y = Marg.adhesion)) + geom_boxplot(alpha = 0.3, color = "#1f78b4") + labs(x = "Class", y = "Marginal Adhesion (scale)", title = "Marginal Adhesion  by Class") + theme(legend.position = "none") + expand_limits(y = 0)

B_E <- ggplot(BC1, aes(x = Class, y = Epith.c.size)) + geom_boxplot(alpha = 0.3, color = "#1f78b4") + labs(x = "Class", y = "Single Epitheial Cell Size (scale)", title = "Single Epitheial Cell Size by Class") + theme(legend.position = "none") + expand_limits(y = 0)

B_BN <- ggplot(BC1, aes(x = Class, y = Bare.nuclei)) + geom_boxplot(alpha = 0.3, color = "#1f78b4") + labs(x = "Class", y = "Bare Nuclei (scale)", title = "Bare Nuclei by Class") + theme(legend.position = "none") + expand_limits(y = 0)

B_BC <- ggplot(BC1, aes(x = Class, y = Bl.cromatin)) + geom_boxplot(alpha = 0.3, color = "#1f78b4") + labs(x = "Class", y = "Bland Cromatin (scale)", title = "Bland Cromatin by Class") + theme(legend.position = "none") + expand_limits(y = 0)

B_NN <- ggplot(BC1, aes(x = Class, y = Normal.nucleoli)) + geom_boxplot(alpha = 0.3, color = "#1f78b4") + labs(x = "Class", y = "Normal Nucleoli (scale)", title = "Normal Nucleoli by Class") + theme(legend.position = "none") + expand_limits(y = 0)

B_M <- ggplot(BC1, aes(x = Class, y = Mitoses)) + geom_boxplot(alpha = 0.3, color = "#1f78b4") + labs(x = "Class", y = "Mitoses (scale)", title = "Mitoses by Class") + theme(legend.position = "none") + expand_limits(y = 0)

# Visualise plots as grid. 
ggarrange(B_CT, B_CS, B_CSh, B_MA, B_E, B_BN, B_BC, B_NN, B_M, ncol=3, nrow=3, common.legend = TRUE, legend="top")
```

\newpage

# Bibliography